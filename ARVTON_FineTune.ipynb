{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARVTON - Virtual Try-On Fine-Tuning\n",
    "\n",
    "This notebook fine-tunes the ARVTON GAN refinement model on Google Colab's free T4 GPU.\n",
    "\n",
    "**Steps:**\n",
    "1. Clone repo & install deps (~3 min)\n",
    "2. Download/prepare dataset (~5 min)\n",
    "3. Train GAN model (~30-60 min for 50 epochs)\n",
    "4. Download checkpoints to local machine\n",
    "\n",
    "---\n",
    "**GPU requirement:** T4 (free tier) or A100 (Pro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment (~3 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "import torch\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the ARVTON repository\n",
    "# Replace with your actual repo URL\n",
    "REPO_URL = \"https://github.com/Gandharv2323/AR-PROTOTYPE-4.git\"  # <-- UPDATE THIS\n",
    "\n",
    "import os\n",
    "if not os.path.exists(\"/content/AR-PROTOTYPE-4\"):\n",
    "    !git clone {REPO_URL} /content/AR-PROTOTYPE-4\n",
    "    print(\"[OK] Repo cloned\")\n",
    "else:\n",
    "    !cd /content/AR-PROTOTYPE-4 && git pull\n",
    "    print(\"[OK] Repo updated\")\n",
    "\n",
    "%cd /content/AR-PROTOTYPE-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (~2 min)\n",
    "!pip install -q -r requirements.txt\n",
    "!pip install -q peft accelerate\n",
    "print(\"[OK] Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directories\n",
    "!python setup_local.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive (for saving checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create checkpoint directory in Drive\n",
    "DRIVE_CKPT = \"/content/drive/MyDrive/ARVTON/checkpoints\"\n",
    "os.makedirs(DRIVE_CKPT, exist_ok=True)\n",
    "print(f\"[OK] Checkpoints will save to: {DRIVE_CKPT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Dataset\n",
    "\n",
    "**Option A:** Upload your own dataset (person + garment image pairs)  \n",
    "**Option B:** Generate synthetic dataset using the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Upload your own dataset\n",
    "# Upload a zip file containing:\n",
    "#   - person_001.jpg, person_002.jpg, ...\n",
    "#   - garment_001.jpg, garment_002.jpg, ...\n",
    "#   - mask_001.png, mask_002.png, ...\n",
    "#   - densepose_001.png, densepose_002.png, ...\n",
    "\n",
    "# Uncomment below to upload from your computer:\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # Upload your dataset.zip\n",
    "# !unzip -q dataset.zip -d data/arvton/datasets/custom/\n",
    "\n",
    "print(\"Skip this cell if using Option B (synthetic data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Generate synthetic dataset (~5-10 min)\n",
    "# This uses Stable Diffusion to create person + garment pairs\n",
    "\n",
    "GENERATE_SYNTHETIC = True  # Set to False if you uploaded your own data\n",
    "NUM_SAMPLES = 100  # Number of training pairs to generate (more = better quality)\n",
    "\n",
    "if GENERATE_SYNTHETIC:\n",
    "    print(f\"Generating {NUM_SAMPLES} synthetic training pairs...\")\n",
    "    print(f\"Estimated time: ~{NUM_SAMPLES * 3}s ({NUM_SAMPLES * 3 / 60:.0f} min)\")\n",
    "    !python -m pipeline.synthetic_gen --target-count {NUM_SAMPLES}\n",
    "    print(\"[OK] Synthetic dataset generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "manifest = Path(\"data/arvton/datasets/train_manifest.json\")\n",
    "if manifest.exists():\n",
    "    with open(manifest) as f:\n",
    "        data = json.load(f)\n",
    "    if isinstance(data, dict) and \"entries\" in data:\n",
    "        count = len(data[\"entries\"])\n",
    "    elif isinstance(data, list):\n",
    "        count = len(data)\n",
    "    else:\n",
    "        count = 0\n",
    "    print(f\"[OK] Training manifest: {count} entries\")\n",
    "else:\n",
    "    print(\"[WARN] No manifest found. Upload data or run synthetic generation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "# Optimized for Colab T4 (16 GB VRAM)\n",
    "\n",
    "EPOCHS = 50           # Total training epochs\n",
    "BATCH_SIZE = 2        # T4: 2, A100: 8\n",
    "LEARNING_RATE = 1e-4  # AdamW learning rate\n",
    "USE_AMP = True        # Mixed precision (saves VRAM)\n",
    "USE_LORA = True       # LoRA adapters (saves VRAM)\n",
    "LORA_RANK = 16        # LoRA rank (higher = more capacity, more VRAM)\n",
    "IMAGE_SIZE = (512, 512)  # T4: 512x512, A100: 768x1024\n",
    "\n",
    "print(f\"Training config:\")\n",
    "print(f\"  Epochs:     {EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  LR:         {LEARNING_RATE}\")\n",
    "print(f\"  AMP:        {USE_AMP}\")\n",
    "print(f\"  LoRA:       {USE_LORA} (rank={LORA_RANK})\")\n",
    "print(f\"  Image size: {IMAGE_SIZE}\")\n",
    "print(f\"\")\n",
    "print(f\"Estimated training time: ~{EPOCHS * 0.8:.0f} min on T4\")\n",
    "print(f\"Estimated VRAM usage:    ~6-8 GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train! (~30-60 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Build the training command\n",
    "cmd = f\"python -m pipeline.train_local --epochs {EPOCHS} --batch-size {BATCH_SIZE} --lr {LEARNING_RATE}\"\n",
    "if USE_AMP:\n",
    "    cmd += \" --amp\"\n",
    "if USE_LORA:\n",
    "    cmd += f\" --lora --lora-rank {LORA_RANK}\"\n",
    "\n",
    "print(f\"Running: {cmd}\")\n",
    "print(f\"Estimated time: ~{EPOCHS * 0.8:.0f} min\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "!{cmd}\n",
    "\n",
    "elapsed = (time.time() - start_time) / 60\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"Training complete in {elapsed:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Checkpoints to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Copy checkpoints to Drive for persistence\n",
    "local_ckpts = Path(\"data/arvton/checkpoints\")\n",
    "if local_ckpts.exists():\n",
    "    for ckpt in local_ckpts.rglob(\"*.pt\"):\n",
    "        dest = Path(DRIVE_CKPT) / ckpt.name\n",
    "        shutil.copy2(str(ckpt), str(dest))\n",
    "        print(f\"  Saved: {dest.name} ({ckpt.stat().st_size / 1024**2:.1f} MB)\")\n",
    "    print(f\"\\n[OK] Checkpoints saved to Google Drive: {DRIVE_CKPT}\")\n",
    "else:\n",
    "    print(\"[WARN] No checkpoints found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Model Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on a sample to check quality\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Find sample images\n",
    "sample_dir = Path(\"data/arvton/datasets\")\n",
    "persons = sorted(sample_dir.rglob(\"person_*.jpg\"))[:3]\n",
    "garments = sorted(sample_dir.rglob(\"garment_*.jpg\"))[:3]\n",
    "\n",
    "if persons and garments:\n",
    "    fig, axes = plt.subplots(len(persons), 3, figsize=(15, 5 * len(persons)))\n",
    "    if len(persons) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, (p, g) in enumerate(zip(persons, garments)):\n",
    "        axes[i, 0].imshow(Image.open(p))\n",
    "        axes[i, 0].set_title(\"Person\")\n",
    "        axes[i, 0].axis(\"off\")\n",
    "        \n",
    "        axes[i, 1].imshow(Image.open(g))\n",
    "        axes[i, 1].set_title(\"Garment\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "        \n",
    "        # Check if a result exists\n",
    "        result_path = Path(f\"data/arvton/outputs/samples/result_{i:03d}.jpg\")\n",
    "        if result_path.exists():\n",
    "            axes[i, 2].imshow(Image.open(result_path))\n",
    "            axes[i, 2].set_title(\"Try-On Result\")\n",
    "        else:\n",
    "            axes[i, 2].text(0.5, 0.5, \"Run inference\\nto see result\", \n",
    "                           ha='center', va='center', fontsize=14)\n",
    "        axes[i, 2].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"data/arvton/outputs/training_samples.png\", dpi=100)\n",
    "    plt.show()\n",
    "    print(\"[OK] Sample visualization saved\")\n",
    "else:\n",
    "    print(\"[WARN] No sample images found for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Download Checkpoint to Local Machine\n",
    "\n",
    "After training, download the best checkpoint to use locally.\n",
    "\n",
    "On your local machine:\n",
    "1. Copy the `.pt` file to `data/arvton/checkpoints/gan/`\n",
    "2. Run: `python run_local.py`\n",
    "3. The backend will automatically use the fine-tuned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the latest checkpoint\n",
    "from google.colab import files\n",
    "\n",
    "ckpts = sorted(Path(DRIVE_CKPT).glob(\"*.pt\"))\n",
    "if ckpts:\n",
    "    latest = ckpts[-1]\n",
    "    size_mb = latest.stat().st_size / 1024**2\n",
    "    print(f\"Downloading: {latest.name} ({size_mb:.1f} MB)\")\n",
    "    print(f\"Estimated download time: ~{size_mb / 5:.0f}s\")\n",
    "    files.download(str(latest))\n",
    "else:\n",
    "    print(\"[WARN] No checkpoints to download. Train first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "After fine-tuning:\n",
    "1. Download the `.pt` checkpoint file\n",
    "2. Place it in your local project at `data/arvton/checkpoints/gan/`\n",
    "3. Start the local backend: `python run_local.py`\n",
    "4. Test with Flutter app: `cd arvton_app && flutter run -d chrome`\n",
    "5. When satisfied, deploy to cloud (Docker + AWS/GCP)"
   ]
  }
 ]
}
